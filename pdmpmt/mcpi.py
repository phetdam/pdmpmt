"""Monte Carlo estimation of pi.

Uses the standard "circle-filling" technique to estimate pi / 4.

.. codeauthor:: Derek Huang <djh458@stern.nyu.edu>
"""

import math
from typing import Any, Iterable, Mapping, Optional, Union

from dask.distributed import Client, LocalCluster, SpecCluster
import numpy as np

# maximum value fo the random seed integers generated by generate_seeds
MAX_SEED_DEFAULT = 89000
# default number of samples to use
N_SAMPLES_DEFAULT = 100000000


def generate_seeds(
    n_seeds: int,
    initial_seed: Optional[int] = None,
    max_seed: int = MAX_SEED_DEFAULT
) -> np.ndarray:
    """Generate an array shape (n_seeds,) of seeds for the NumPy PRNG.

    Seed values are selected with replacement from [0, max_seed).

    n_seeds : int
        Number of seed values to generate
    initial_seed : int, default=None
        Seed for NumPy PRNG for reproducibility
    max_seed : int, default=MAX_SEED_DEFAULT
        Upper bound for the generated seed values
    """
    rng = np.random.default_rng(initial_seed)
    return rng.integers(max_seed, high=None, size=n_seeds)


def generate_sample_counts(n_samples: int, n_jobs: int) -> np.ndarray:
    """Generate number of samples each job is responsible for.

    Parameters
    ----------
    n_samples : int
        Number of samples in total to be drawn
    n_jobs : int
        Number of jobs to split the samples over
    """
    # number per batch is at least floor(n_samples / n_jobs), we spread the
    # remainder out over each of the jobs evenly
    sample_counts = np.full(n_jobs, n_samples // n_jobs)
    sample_counts[:n_samples % n_jobs] += 1
    return sample_counts


def unit_circle_samples(n_samples: int, seed: Optional[int] = None) -> int:
    """Return number of samples that fall inside the unit circle.

    This function performs the majority of the work needed to estimate pi.

    Parameters
    ----------
    n_samples : int
        Number of samples to draw from [-1, 1] X [-1, 1]
    seed : int, default=None
        Seed for NumPy PRNG for reproducibility
    """
    rng = np.random.default_rng(seed)
    points = rng.uniform(low=-1., high=1., size=(n_samples, 2))
    norms = np.linalg.norm(points, axis=1)
    return norms[norms <= 1.].size


def mcpi_gather(
    circle_counts: Iterable[int], sample_counts: Iterable[int]
) -> float:
    """Estimate pi from computed circle and sample counts.

    Parameters
    ----------
    circle_counts : Iterable[int]
        Iterable of counts of samples per batch falling in the unit circle
    sample_counts : Iterable[int]
        Iterable of the total samples per batch
    """
    # use int64 and divide the two counts first to reduce chance of overflow
    return 4 * (
        np.sum(circle_counts, dtype=np.int64) /
        np.sum(sample_counts, dtype=np.int64)
    )


def mcpi_serial(
    n_samples: int = N_SAMPLES_DEFAULT, seed: Optional[int] = None
) -> float:
    """Serial estimation of pi using Monte Carlo.

    This can be considered the "optimal" single-process Python implementation.

    Parameters
    ----------
    n_samples : int, default=N_SAMPLES_DEFAULT
        Number of samples to draw
    seed : int, default=None
        Seed for NumPy PRNG for reproducibility
    """
    # again divide the two integers before multiplying by 4 to avoid overflow
    return 4 * (unit_circle_samples(n_samples, seed=seed) / n_samples)


def mcpi_dask(
    n_samples: int = N_SAMPLES_DEFAULT,
    seed: Optional[int] = None,
    cluster: Optional[Union[str, SpecCluster]] = None,
    n_jobs: Optional[int] = None,
    n_workers: Optional[int] = None,
    verbose: bool = False,
    client_kwargs: Optional[Mapping[str, Any]] = None,
    local_cluster_kwargs: Optional[Mapping[str, Any]] = None
):
    """Parallel estimation of pi using Monte Carlo with Dask.

    Parallel
    --------
    n_samples : int, default=N_SAMPLES_DEFAULT
        Number of samples to draw
    seed : int, default=None
        Seed for NumPy PRNG for reproducibility
    cluster : str or SpecCluster, default=None
        Dask cluster object or address of cluster scheduler to connect to. If
        not provided, a LocalCluster will be created with n_workers worker
        processes and will be killed at the end of the function call.
    n_jobs : int, default=None
        Number of jobs to submit to the Dask cluster. If not specified,
        defaults to either n_workers, number of workers in the new LocalCluster
        if cluster=None, or ceil(n_samples / 0.5e8).
    n_workers : int, default=None
        Number of worker processes to use. Also passed to the LocalCluster that
        is created when cluster=None, in which case if n_workers=None, the
        number of real available cores on the local machine is used.
    client_kwargs : Mapping[str, Any], default=None
        Extra keyword args to pass to the Client __init__ method
    local_cluster_kwargs : Mapping[str, Any], default=None
        Extra keyword args to pass to the LocalCluster __init__method when
        cluster=None and a LocalCluster is created by this function
    """
    if not client_kwargs:
        client_kwargs = {}
    if not local_cluster_kwargs:
        local_cluster_kwargs = {}
    # use local multi-worker cluster if no cluster/scheduler address
    if not cluster:
        _cluster = LocalCluster(n_workers=n_workers, **local_cluster_kwargs)
    else:
        _cluster = cluster
    # no need to make this cluster the default compute method
    client = Client(_cluster, set_as_default=False, **client_kwargs)
    # if number of jobs is unknown, set it number of 10 * n_workers. but if
    # n_workers is not set, use LocalCluster workers, else choose number of
    # jobs such that the batch size won't exceed 0.4G (0.5e8 samples)
    if not n_jobs:
        if n_workers:
            n_jobs = n_workers
        elif isinstance(client.cluster, LocalCluster):
            n_jobs = len(client.cluster.workers)
        else:
            n_jobs = math.ceil(n_samples / 0.5e8)
    if verbose:
        print(f"n_jobs={n_jobs}, n_workers={n_workers}")
        print(f"cluster={_cluster}")
    # split tasks in n_jobs jobs, generating seeds and circle counts
    seeds = generate_seeds(n_jobs, initial_seed=seed)
    sample_counts = generate_sample_counts(n_samples, n_jobs)
    if verbose:
        print(f"seeds={seeds}")
        print(f"sample_counts={sample_counts}")
    # map unit_circle_samples to perform sampling over seeds, sample_counts
    futs = client.map(unit_circle_samples, sample_counts, seeds, pure=False)
    circle_counts = client.gather(futs)
    # close client; if LocalCluster was created, it is also closed
    client.close()
    if not cluster:
        client.cluster.close()
    # gather circle counts and sample counts to estimate pi
    return mcpi_gather(circle_counts, sample_counts)
